{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":47283,"sourceType":"datasetVersion","datasetId":34683}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Pix2Pix GAN to generate maps from satellite imagery \n\n#### In this project, we explore the power of Generative Adversarial Networks (GANs) to perform image-to-image translation, a task often referred to as \"Pix2Pix.\" GANs have revolutionized the field of computer vision and image processing by enabling us to generate realistic images from input data. Pix2Pix, a specific GAN variant, excels at transforming images from one domain to another, making it an exciting tool for various applications.\n\n#### Image-to-image translation is a fascinating challenge in the realm of computer vision. It involves converting an image from one style, domain, or representation to another. For instance, we can transform black-and-white sketches into colorful images, turn satellite images into maps, or change summer photos into winter scenes. Pix2Pix GANs have shown remarkable capabilities in solving these problems by learning the mapping between input and output domains.\n![download.png](attachment:da44c4d5-ccce-4054-b540-4f22122a3487.png)\n\n#### Pix2Pix consists of two neural networks: a generator and a discriminator. The generator takes an input image from one domain and tries to produce an output image in the desired target domain. Simultaneously, the discriminator evaluates whether the generated image is realistic or not. Through adversarial training, the generator learns to improve its output, making it increasingly difficult for the discriminator to distinguish between real and generated images.\n![download.png](attachment:a98b3122-c7d0-469b-9d93-9f9e6f881539.png)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:38:11.763100Z","iopub.execute_input":"2024-05-11T12:38:11.763354Z","iopub.status.idle":"2024-05-11T12:38:13.713883Z","shell.execute_reply.started":"2024-05-11T12:38:11.763331Z","shell.execute_reply":"2024-05-11T12:38:13.712624Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/bin/bash: -c: line 1: syntax error near unexpected token `attachment:da44c4d5-ccce-4054-b540-4f22122a3487.png'\n/bin/bash: -c: line 1: `[download.png](attachment:da44c4d5-ccce-4054-b540-4f22122a3487.png)'\n/bin/bash: -c: line 1: syntax error near unexpected token `attachment:a98b3122-c7d0-469b-9d93-9f9e6f881539.png'\n/bin/bash: -c: line 1: `[download.png](attachment:a98b3122-c7d0-469b-9d93-9f9e6f881539.png)'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Impoting relevant libraries","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom glob import glob\nimport time\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.models import Model\nfrom keras.layers import Conv2D, Input, MaxPool2D, Conv2DTranspose, concatenate, Lambda, BatchNormalization, Activation, LeakyReLU, ReLU\nfrom keras.utils import img_to_array, load_img, plot_model\nfrom keras.optimizers import Adam\nfrom keras.initializers import RandomNormal","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:38:13.716469Z","iopub.execute_input":"2024-05-11T12:38:13.716844Z","iopub.status.idle":"2024-05-11T12:38:22.117631Z","shell.execute_reply.started":"2024-05-11T12:38:13.716803Z","shell.execute_reply":"2024-05-11T12:38:22.116785Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Reading the data, and seeing the training images\n\n#### The dataset contains satellite images along with the annotated images that look like a map version of our satellite image. The satellite and the target images are both given in one single image side by side.","metadata":{}},{"cell_type":"code","source":"path = \"../input/pix2pix-dataset/maps/maps/train/\"\nnum_images = 1000\n\ncombined_images = sorted(glob(path + \"*.jpg\"))[:num_images]\n    \nimages = np.zeros(shape=(len(combined_images), 256, 256, 3))\nmasks = np.zeros(shape=(len(combined_images), 256, 256, 3))\n\nfor idx, path in enumerate(combined_images):\n\n    combined_image = tf.cast(img_to_array(load_img(path)), tf.float32)\n    \n    image = combined_image[:,:600,:]\n    mask = combined_image[:,600:,:]\n\n    images[idx] = (tf.image.resize(image,(256,256)))/255\n    masks[idx] = (tf.image.resize(mask,(256,256)))/255\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:38:22.118657Z","iopub.execute_input":"2024-05-11T12:38:22.119187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(25,10))\nfor i in range(1,6):\n    idx = np.random.randint(1,1000)\n    image, mask = images[idx], masks[idx]\n    plt.subplot(2,5,i)\n    plt.imshow(image)\n    plt.title(str(i) + \" .Satellite image\")\n    plt.axis(\"off\")\n    \n    plt.subplot(2,5,i + 5)\n    plt.imshow(mask)\n    plt.title(str(i) + \" .Map \")\n    plt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the GAN\n\n### Creating the Generator of the model\n#### The generator in the Pix2Pix GAN is a U-net","metadata":{}},{"cell_type":"code","source":"def downscale(num_filters):\n    block = Sequential()\n    block.add(Conv2D(num_filters, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False))\n    block.add(LeakyReLU(alpha=0.2))\n    block.add(BatchNormalization())\n    return block\n\ndef upscale(num_filters):\n    block = Sequential()\n    block.add(Conv2DTranspose(num_filters, kernel_size=4, strides=2, padding='same', kernel_initializer='he_normal', use_bias=False))\n    block.add(LeakyReLU(alpha=0.2))\n    block.add(BatchNormalization())\n    block.add(ReLU())\n    return block\n\ndef Generator():\n    inputs = Input(shape=(256,256,3), name=\"InputLayer\")\n\n    encoder = [\n        downscale(64),\n        downscale(128),\n        downscale(256),\n        downscale(512),\n        downscale(1024),\n        downscale(1024),\n    ]\n    \n    latent_space = downscale(1024)\n\n    decoder = [\n        upscale(1024),\n        upscale(1024),\n        upscale(512),\n        upscale(256),\n        upscale(128),\n        upscale(64),\n    ]\n    \n    x = inputs \n    skips = []\n    for layer in encoder:\n        x = layer(x)\n        skips.append(x)\n    \n    x = latent_space(x)\n\n    skips = reversed(skips)\n    for up, skip in zip(decoder, skips):\n        x = up(x)\n        x = concatenate([x, skip])\n    \n    initializer = RandomNormal(stddev=0.02, seed=42)\n    outputs = Conv2DTranspose(3, kernel_size=4, strides=2, kernel_initializer = initializer, activation = 'tanh', padding = 'same')\n\n    outputs = outputs(x)\n\n    generator = Model(inputs = inputs, outputs = outputs, name=\"Generator\")\n    return generator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = Generator()\nplot_model(generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Discriminator of the model\n\n#### The discrimintor in the Pix2Pix GAN is a Patch Gan","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    image = Input(shape = (256,256,3), name = \"ImageInput\")\n    target = Input(shape = (256,256,3), name = \"TargetInput\")\n    x = concatenate([image, target])\n\n    x = downscale(64)(x)\n    x = downscale(128)(x)\n    x = downscale(512)(x)\n\n    initializer = RandomNormal(stddev = 0.02, seed=42)\n        \n    x = Conv2D(512, kernel_size = 4, strides = 1, kernel_initializer = initializer, use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n\n    x = Conv2D(1, kernel_size = 4, kernel_initializer = initializer)(x)\n\n    discriminator = Model(inputs = [image, target], outputs = x, name = \"Discriminator\")\n\n    return discriminator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator = Discriminator()\nplot_model(discriminator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating loss functions, and training functions\n\n#### The GAN is a bit different form other DL models, here we just cannot call the model.fit method to start the training process. The loss is passed in a non-sequential fashion here. And to calculate the loss, we make use of the Tensorflow's automatic differentiation engine, that calculates gradients and then we can use the optimizers to perform gradient descent.","metadata":{}},{"cell_type":"code","source":"adversarial_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\ngenerator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndef generator_loss(discriminator_generated, generated_output, target_image):\n    gan_loss = adversarial_loss(tf.ones_like(discriminator_generated), discriminator_generated)\n    l1_loss = tf.reduce_mean(tf.abs(target_image - generated_output))\n    total_loss = (100 * l1_loss) + gan_loss\n    return total_loss, gan_loss, l1_loss\n\ndef discriminator_loss(discriminator_real_output, discriminator_generated_output):\n    real_loss = adversarial_loss(tf.ones_like(discriminator_real_output), discriminator_real_output)\n    fake_loss = adversarial_loss(tf.zeros_like(discriminator_generated_output), discriminator_generated_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef train_step(inputs, target):\n    with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n        generated_output = generator(inputs, training=True)\n        \n        discriminator_real_output = discriminator([inputs, target], training=True)\n        discriminator_generated_output = discriminator([inputs, generated_output], training=True)\n        \n        generator_total_loss, generator_gan_loss, generator_l1_loss = generator_loss(discriminator_generated_output, generated_output, target)\n        \n        discriminator_Loss = discriminator_loss(discriminator_real_output, discriminator_generated_output)\n        \n    generator_gradients = generator_tape.gradient(generator_total_loss, generator.trainable_variables)\n    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n    \n    discriminator_gradients = discriminator_tape.gradient(discriminator_Loss, discriminator.trainable_variables)\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(data, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n    print(\"Current epoch: \", epoch+1)\n    for image, mask in data:\n      train_step(image, mask)\n    print(f\"Time taken to complete the epoch {epoch + 1} is {(time.time() - start):.2f} seconds \\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sat_image, map_image = tf.cast(images, tf.float32), tf.cast(masks, tf.float32)\ndataset = (sat_image,map_image)\ndata = tf.data.Dataset.from_tensor_slices(dataset).batch(32, drop_remainder=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit(data, 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_predictions(num_samples):\n    for i in range(num_samples):\n        idx = np.random.randint(images.shape[0])\n        image, mask = images[idx], masks[idx]\n        predicted = generator.predict(tf.expand_dims(image, axis=0))[0]\n        \n        plt.figure(figsize=(10,8))\n        \n        plt.subplot(1,3,1)\n        plt.imshow(image)\n        plt.title(\"Satellite Image \" + str(idx))\n        plt.axis('off')\n        \n        plt.subplot(1,3,2)\n        plt.imshow(mask)\n        plt.title(\"Map Image \" + str(idx))\n        plt.axis('off')\n        \n        plt.subplot(1,3,3)\n        plt.imshow(predicted)\n        plt.title(\"Predicted Image \" + str(idx))\n        plt.axis('off')\n        \n        plt.show()\n        \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_predictions(5)\nimage, mask = images[621], masks[621]\npredicted = generator.predict(tf.expand_dims(image, axis=0))[0]\n        \nplt.figure(figsize=(10,8))\n        \nplt.subplot(1,3,1)\nplt.imshow(image)\nplt.title(\"Satellite Image \" + \"This\")\nplt.axis('off')\n        \nplt.subplot(1,3,2)\nplt.imshow(mask)\nplt.title(\"Map Image \" + \"This\")\nplt.axis('off')\n        \nplt.subplot(1,3,3)\nplt.imshow(predicted)\nplt.title(\"Predicted Image \" + \"This\")\nplt.axis('off')\n        \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport math\n\ndef calculate_metrics(images, masks, generator, num_samples):\n    mse_scores = []\n    mae_scores = []\n\n    for i in range(num_samples):\n        idx = np.random.randint(images.shape[0])\n        image, mask = images[idx], masks[idx]\n        predicted = generator.predict(np.expand_dims(image, axis=0))[0]\n        \n        # Calculate MSE\n        mse_score = np.mean((mask - predicted) ** 2)\n        mse_scores.append(mse_score)\n        \n        # Calculate MAE\n        mae_score = np.mean(np.abs(mask - predicted))\n        mae_scores.append(mae_score)\n        \n    avg_mse = np.mean(mse_scores)\n    avg_mae = np.mean(mae_scores)\n    \n    return avg_mse, avg_mae\n\n# Usage\nnum_samples = 100  # Number of samples to evaluate\navg_mse, avg_mae = calculate_metrics(images, masks, generator, num_samples)\nprint(\"Average MSE:\", avg_mse * 100)\nprint(\"Average MAE:\", avg_mae * 100)\nprint(\"Average RMSE:\",math.sqrt(avg_mse) * 100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport math\n\ndef calculate_metrics(images, masks, generator, num_samples):\n    mse_scores = []\n    mae_scores = []\n\n    for i in range(num_samples):\n        idx = np.random.randint(images.shape[0])\n        image, mask = images[idx], masks[idx]\n        predicted = generator.predict(np.expand_dims(image, axis=0))[0]\n        \n        # Calculate MSE\n        mse_score = np.mean((mask - predicted) ** 2)\n        mse_scores.append(mse_score)\n        \n        # Calculate MAE\n        mae_score = np.mean(np.abs(mask - predicted))\n        mae_scores.append(mae_score)\n        \n    avg_mse = np.mean(mse_scores)\n    avg_mae = np.mean(mae_scores)\n    \n    return avg_mse, avg_mae\n\n# Usage\nnum_samples = 100  # Number of samples to evaluate\navg_mse, avg_mae = calculate_metrics(images, masks, generator, num_samples)\nprint(\"Average MSE:\", avg_mse * 100)\nprint(\"Average MAE:\", avg_mae * 100)\nprint(\"Average RMSE:\",math.sqrt(avg_mse) * 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The results are decent, one can train the model for more epochs to get better results, and tweak the architecture to get much better results.","metadata":{}},{"cell_type":"code","source":"generator.save(\"GAN_Sat_image_2_map.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}